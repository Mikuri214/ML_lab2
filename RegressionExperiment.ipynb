from sklearn.datasets import load_svmlight_file as load
import numpy as np
import matplotlib.pyplot as plt



def compute_loss(w, x, y):
    n = x.shape[0]
    total = 0
    for z in range(n):
        total += np.log(1 + np.exp(np.sum(-y[z]*x[z]*w)))
    loss = lamb * np.sum(np.square(w)) / 2 + total/n
    print(loss)
    return loss


def NAG(w, v_w, mu):
    for i in range(iteration):
        num = 0
        w_ahead = w + mu * v_w
        w_gradient = np.zeros((x_train.shape[1],1))

        # parts of samples
        pad = np.random.randint(1, 5)
        for j in range(0, x_train.shape[0], pad):
            num += 1
            temp = 1 + np.exp(np.sum(y_train[j] * x_train[j] * w_ahead))
            w_gradient += np.sum(y_train[j] / temp)* x_train[j].T

        dw = learning_rate * lamb * w_ahead - learning_rate * w_gradient / num
        v_w = mu * v_w - learning_rate * dw
        w += v_w
        NAG_validation_loss.append(compute_loss(w, x_validation, y_validation))


def RMSprop(w, eps, decay_rate, cache_w):
    for i in range(iteration):
        num = 0
        w_gradient = np.zeros((x_train.shape[1],1))

        # parts of samples
        pad = np.random.randint(1, 5)
        for j in range(0, x_train.shape[0], pad):
            num += 1
            temp = (1 + np.exp(np.sum(y_train[j] * x_train[j] * w)))
            w_gradient += np.sum(y_train[j] / temp) * x_train[j].T

        dw = learning_rate * lamb * w - learning_rate * w_gradient / num
        cache_w = decay_rate * cache_w + (1 - decay_rate) * np.square(dw)
        w += - learning_rate * dw / (np.sqrt(cache_w) + eps)

        RMSProp_validation_loss.append(compute_loss(w, x_validation, y_validation))


def Adam(w, eps, m_w, v_w, beta1=0.9, beta2=0.999):
    for i in range(iteration):
        w_gradient = np.zeros((x_train.shape[1],1))
        num = 0

        # parts of samples
        pad = np.random.randint(1, 5)
        for j in range(0, x_train.shape[0], pad):
            num += 1
            temp = (1 + np.exp(np.sum(y_train[j] * x_train[j] * w)))
            w_gradient += np.sum(y_train[j] / temp) * x_train[j].T

        dw = learning_rate * lamb * w - learning_rate * w_gradient / num
        m_w = beta1 * m_w + (1 - beta1) * dw
        v_w = beta2 * v_w + (1 - beta2) * np.square(dw)
        w += - learning_rate * m_w / (np.sqrt(v_w) + eps)

        Adam_validation_loss.append(compute_loss(w, x_validation, y_validation))


def AdaDelta(w, eps, n_steps = 50, decay_rate = 0.9 ):
    for i in range(iteration):
        mean_gradient_w = np.zeros((x_train.shape[1], 1))
        mean_step_w = np.zeros((x_train.shape[1], 1))
        w_gradient = np.zeros((x_train.shape[1],1))
        num = 0

        # parts of samples
        pad = np.random.randint(1, 5)
        for j in range(0, x_train.shape[0], pad):
            num += 1
            temp = (1 + np.exp(np.sum(y_train[j] * x_train[j] * w)))
            w_gradient += np.sum(y_train[j] / temp) * x_train[j].T

        dw = learning_rate * lamb * w - learning_rate * w_gradient / num

        for k in range(n_steps):
            mean_gradient_w = decay_rate * mean_gradient_w + (1 - decay_rate) * np.square(dw)
            temp = np.sqrt(((mean_step_w + eps) / (mean_gradient_w + eps)))
            steps_w = -np.multiply(temp, dw)
            mean_step_w = decay_rate * mean_step_w + (1 - decay_rate) * np.square(steps_w)
            w += steps_w

        AdaDelta_validation_loss.append(compute_loss(w, x_validation, y_validation))


#================================= load data ================================

train_data = load('./a9a')
test_data = load('./a9a.t',n_features=123)

x_train, y_train = train_data[0], train_data[1]
x_validation, y_validation = test_data[0], test_data[1]
x_train,x_validation,y_train,y_validation = x_train.todense(),x_validation.todense(),y_train.reshape(len(y_train),-1),y_validation.reshape(len(y_validation),-1)



init_w = np.ones((x_train.shape[1],1))

iteration = 200
learning_rate = 0.1
lamb = 0.9
NAG_validation_loss=[]
RMSProp_validation_loss=[]
Adam_validation_loss=[]
AdaDelta_validation_loss=[]



#for NAG
print('\nNAG')
NAG(init_w, v_w=0, mu=0.9 * np.ones((x_train.shape[1],1)))

#for RMSProp
print('\nRMSProp')
RMSprop(init_w, decay_rate =0.9, eps = 1e-6 * np.ones((x_train.shape[1],1)),
        cache_w = np.empty((x_train.shape[1],1)) )

#for Adam
print('\nAdam')
Adam(init_w,eps=1e-8 * np.ones((x_train.shape[1],1)), m_w = np.empty((x_train.shape[1],1)),
     v_w = np.ones((x_train.shape[1],1)), beta1=0.9, beta2=0.999)

#for AdaDelta
print('\nAdaDelta')
AdaDelta(init_w, eps = 1e-8 * np.ones((x_train.shape[1],1)), n_steps = 50, decay_rate = 0.9)



t = np.arange(0, iteration, 1)
plt.plot(t, NAG_validation_loss, color="red", linewidth=2.5, linestyle="-", label="NAG_validation_loss")
plt.plot(t, RMSProp_validation_loss, color="blue",  linewidth=2.5, linestyle="-", label="RMSProp_validation_loss")
plt.plot(t, Adam_validation_loss, color="green", linewidth=2.5, linestyle="-", label="Adam_validation_loss")
plt.plot(t, AdaDelta_validation_loss, color="yellow", linewidth=2.5, linestyle="-", label="AdaDelta_validation_loss")
plt.legend(loc='upper right')
plt.plot(t, NAG_validation_loss, 'r--',t, RMSProp_validation_loss, 'b--',t,Adam_validation_loss,'g--',t,AdaDelta_validation_loss,'y--')

plt.xlabel('iteration')
plt.ylabel('loss')
plt.show()