from sklearn.datasets import load_svmlight_file as load
import numpy as np
import matplotlib.pyplot as plt

#define

def compute_loss(w, b, x, y):
    n = x.shape[1]
    total = 0
    for z in range(x.shape[0]):
        if np.sum((1 - y[z] * (x[z] * w + b[z]))) > 0:
            total += np.sum((1 - y[z] * (x[z] * w + b[z])))
    loss = np.sum(np.square(w)) / (2*n) + C * total
    print(loss)
    return loss


def NAG(w, b, v_w, v_b, mu):
    for i in range(iteration):
        dw = np.zeros((x_train.shape[1],1))
        db = 0
        w_ahead = w + mu * v_w
        b_ahead = b + mu * v_b * np.ones((x_train.shape[0],1))

        for j in range(0, x_train.shape[0]):
            if np.sum((1 - y_train[j] * (x_train[j] * w_ahead + b[j]))) > 0:
                w_gradient = x_train[j].T * (-1 * y_train[j])
            else:
                w_gradient = 0
            if np.sum((1 - y_train[j] * (x_train[j] * w + b_ahead[j]))) > 0:
                b_gradient = -y_train[j]
            else:
                b_gradient = 0

            dw = w_ahead + C * w_gradient
            db = C * b_gradient

        v_w = mu * v_w - learning_rate * dw
        v_b = mu * v_b - learning_rate * db
        w += v_w
        b += v_b * np.ones((x_train.shape[0],1))

        NAG_validation_loss.append(compute_loss(w, b, x_validation, y_validation))


def RMSprop(w, b, eps, decay_rate, cache_w, cache_b):
    for i in range(iteration):
        dw = np.zeros((x_train.shape[1],1))
        db = 0

        for j in range(0, x_train.shape[0]):
            if np.sum((1 - y_train[j] * (x_train[j] * w + b[j]))) > 0:
                w_gradient = x_train[j].T * (-1 * y_train[j])
                b_gradient = -y_train[j]
            else:
                w_gradient = 0
                b_gradient = 0

            dw = w + C * w_gradient
            db = C * b_gradient

        cache_w = decay_rate * cache_w + (1 - decay_rate) * np.square(dw)
        w += - learning_rate * dw / (np.sqrt(cache_w) + eps)

        cache_b = decay_rate * cache_b + (1 - decay_rate) * db ** 2
        b += - learning_rate * db / (np.sqrt(cache_b) + eps[0]) * np.ones((x_train.shape[0],1))

        RMSProp_validation_loss.append(compute_loss(w, b, x_validation, y_validation))


def Adam(w, b, eps, m_w, v_w, m_b, v_b, beta1=0.9, beta2=0.999):
    for i in range(iteration):
        dw = np.zeros((x_train.shape[1],1))
        db = 0

        for j in range(0, x_train.shape[0]):
            if np.sum((1 - y_train[j] * (x_train[j] * w + b[j]))) > 0:
                w_gradient = x_train[j].T * (-1 * y_train[j])
                b_gradient = -y_train[j]
            else:
                w_gradient = 0
                b_gradient = 0

            dw = w + C * w_gradient
            db = C * b_gradient

        m_w = beta1 * m_w + (1 - beta1) * dw
        v_w = beta2 * v_w + (1 - beta2) * np.square(dw)

        m_b = beta1 * m_b + (1 - beta1) * db
        v_b = beta2 * v_b + (1 - beta2) * db ** 2

        w += - learning_rate * m_w / (np.sqrt(v_w) + eps)
        b += - learning_rate * m_b / (np.sqrt(v_b) + eps[0]) * np.ones((x_train.shape[0],1))

        Adam_validation_loss.append(compute_loss(w, b, x_validation, y_validation))


def AdaDelta(w, b, eps, n_steps = 50, decay_rate = 0.9 ):
    for i in range(iteration):
        mean_gradient_w = np.zeros((x_train.shape[1], 1))
        mean_step_w = np.zeros((x_train.shape[1], 1))
        mean_gradient_b = 0
        mean_step_b = 0
        dw = np.zeros((x_train.shape[1],1))
        db = 0

        for j in range(0, x_train.shape[0]):
            if np.sum((1 - y_train[j] * (x_train[j] * w + b[j]))) > 0:
                w_gradient = x_train[j].T * (-1 * y_train[j])
                b_gradient = -y_train[j]
            else:
                w_gradient = 0
                b_gradient = 0

            dw = w + C * w_gradient
            db = C * b_gradient

        for k in range(n_steps):
            mean_gradient_w = decay_rate * mean_gradient_w + (1 - decay_rate) * np.square(dw)
            temp = np.sqrt(((mean_step_w + eps) / (mean_gradient_w + eps)))
            steps_w = -np.multiply(temp, dw)
            mean_step_w = decay_rate * mean_step_w + (1 - decay_rate) * np.square(steps_w)
            w += steps_w

            mean_gradient_b = decay_rate * mean_gradient_b + (1 - decay_rate) * np.square(db)
            temp = np.sqrt(((mean_step_b + eps[0]) / (mean_gradient_b + eps[0])))
            steps_b = -np.multiply(temp, db)
            mean_step_b = decay_rate * mean_step_b + (1 - decay_rate) * np.square(steps_b)
            b += steps_b * np.ones((x_train.shape[0],1))

        AdaDelta_validation_loss.append(compute_loss(w, b, x_validation, y_validation))


#load

train_data = load('./data//a9a')
test_data = load('./data/a9a.t',n_features=123)

x_train, y_train = train_data[0], train_data[1]
x_validation, y_validation = test_data[0], test_data[1]
x_train,x_validation,y_train,y_validation = x_train.todense(),x_validation.todense(),y_train.reshape(len(y_train),-1),y_validation.reshape(len(y_validation),-1)

#initialize

init_b = np.zeros((x_train.shape[0],1))
init_w = np.ones((x_train.shape[1],1))

iteration = 100
learning_rate = 0.01
C = 0.01
NAG_validation_loss=[]      #记录每次迭代的NAG的loss值
RMSProp_validation_loss=[]  #记录每次迭代的RMSProp的loss值
Adam_validation_loss=[]     #记录每次迭代的Adam的loss值
AdaDelta_validation_loss=[] #记录每次迭代的AdaDelta的loss值

#train

#for NAG
print('\nNAG')
NAG(init_w, init_b, v_w = np.zeros((x_train.shape[1],1)), v_b=0, mu=0.9 )

#for RMSProp
print('\nRMSProp')
RMSprop(init_w, init_b, decay_rate =0.9, eps = 1e-6 * np.ones((x_train.shape[1],1)),
        cache_w = np.empty((x_train.shape[1],1)), cache_b = 0)

#for Adam
print('\nAdam')
Adam(init_w, init_b, eps=1e-8 * np.ones((x_train.shape[1],1)), m_w = np.zeros((x_train.shape[1],1)),
     v_w = np.zeros((x_train.shape[1],1)), m_b = 0, v_b = 0 ,beta1=0.9, beta2=0.999)

#for AdaDelta
print('\nAdaDelta')
AdaDelta(init_w, init_b, eps = 1e-8 * np.ones((x_train.shape[1],1)), n_steps = 50, decay_rate = 0.9)


#show

t = np.arange(0, iteration, 1)
plt.plot(t, NAG_validation_loss, color="red", linewidth=2.5, linestyle="-", label="NAG_validation_loss")
plt.plot(t, RMSProp_validation_loss, color="blue",  linewidth=2.5, linestyle="-", label="RMSProp_validation_loss")
plt.plot(t, Adam_validation_loss, color="green", linewidth=2.5, linestyle="-", label="Adam_validation_loss")
plt.plot(t, AdaDelta_validation_loss, color="yellow", linewidth=2.5, linestyle="-", label="AdaDelta_validation_loss")
plt.legend(loc='upper right')
plt.plot(t, NAG_validation_loss, 'r--',t, RMSProp_validation_loss, 'b--',t,Adam_validation_loss,'g--',t,AdaDelta_validation_loss,'y--')
#plt.plot(t, NAG_validation_loss, 'r--')
plt.xlabel('iteration')
plt.ylabel('loss')
plt.show()
